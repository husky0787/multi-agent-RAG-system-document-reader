{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install docling langchain-text-splitters chromadb sentence-transformers rank-bm25 openai gradio langgraph"
      ],
      "metadata": {
        "id": "3Q0fIkLPVR-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "from typing import TypedDict, List, Dict\n",
        "from docling.document_converter import DocumentConverter\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "from rank_bm25 import BM25Okapi\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# LLM Client\n",
        "client = OpenAI(\n",
        "    api_key=\"R9BJEe5Zj5a4f3uPxtzzQaU-e6CedJfd6duFJnNgFVU\",\n",
        "    base_url=\"https://api.poe.com/v1\",\n",
        ")\n",
        "\n",
        "def llm_call(system_prompt: str, user_prompt: str, model: str = \"GPT-4o-mini\") -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Document Processing\n",
        "def get_cache_key(file_path: str) -> str:\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "def process_document(file_path: str) -> List[str]:\n",
        "    if os.path.getsize(file_path) > 10 * 1024 * 1024:  # 10 MB limit\n",
        "        raise ValueError(f\"File {file_path} is too large (max 10MB).\")\n",
        "\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext in ['.txt', '.md']:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            md_content = f.read()\n",
        "    else:\n",
        "        converter = DocumentConverter()\n",
        "        result = converter.convert(file_path)\n",
        "        md_content = result.document.export_to_markdown()\n",
        "\n",
        "    # Split into chunks based on headers\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "    ]\n",
        "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    docs = splitter.split_text(md_content)\n",
        "    chunks = [doc.page_content for doc in docs]\n",
        "    return chunks\n",
        "\n",
        "# Hybrid Retriever\n",
        "class HybridRetriever:\n",
        "    def __init__(self, chunks: List[str]):\n",
        "        self.chunks = chunks\n",
        "        tokenized_chunks = [chunk.split(\" \") for chunk in chunks]\n",
        "        self.bm25 = BM25Okapi(tokenized_chunks)\n",
        "        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
        "        self.chroma_client = chromadb.Client()\n",
        "        self.collection = self.chroma_client.get_or_create_collection(name=\"doc_chunks\")\n",
        "        self.collection.add(\n",
        "            documents=chunks,\n",
        "            ids=[str(i) for i in range(len(chunks))],\n",
        "            embeddings=self.embedding_function(chunks),  # Precompute embeddings\n",
        "        )\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5) -> List[str]:\n",
        "        # BM25 retrieval\n",
        "        bm25_scores = self.bm25.get_scores(query.split(\" \"))\n",
        "        bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
        "\n",
        "        # Vector retrieval\n",
        "        query_embedding = self.embedding_function([query])[0]\n",
        "        vector_results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=top_k,\n",
        "        )\n",
        "        vector_indices = [int(id) for id in vector_results['ids'][0]]\n",
        "\n",
        "        # Combine and deduplicate\n",
        "        combined_indices = list(set(bm25_indices + vector_indices))\n",
        "        retrieved_chunks = [self.chunks[i] for i in combined_indices]\n",
        "        return retrieved_chunks\n",
        "\n",
        "# LangGraph State\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    documents: List[str]\n",
        "    relevance: str\n",
        "    answer: str\n",
        "    verification: str\n",
        "\n",
        "# Nodes\n",
        "def retrieve_documents(state: State) -> Dict:\n",
        "    return {\"documents\": retriever.retrieve(state[\"question\"])}\n",
        "\n",
        "def check_relevance(state: State) -> Dict:\n",
        "    context = \"\\n\\n\".join(state[\"documents\"])\n",
        "    system_prompt = \"You are a relevance checker. Classify if the provided context can answer the question.\"\n",
        "    user_prompt = f\"Question: {state['question']}\\nContext: {context}\\nOutput ONLY one of: CAN_ANSWER (fully answers), PARTIAL (partially answers), NO_MATCH (cannot answer).\"\n",
        "    relevance = llm_call(system_prompt, user_prompt)\n",
        "    return {\"relevance\": relevance.strip()}\n",
        "\n",
        "def generate_answer(state: State) -> Dict:\n",
        "    context = \"\\n\\n\".join(state[\"documents\"])\n",
        "    system_prompt = \"You are a research agent. Provide an accurate answer based ONLY on the provided context. Do not add external knowledge.\"\n",
        "    user_prompt = f\"Question: {state['question']}\\nContext: {context}\\nAnswer the question concisely.\"\n",
        "    answer = llm_call(system_prompt, user_prompt)\n",
        "    return {\"answer\": answer}\n",
        "\n",
        "def verify_answer(state: State) -> Dict:\n",
        "    context = \"\\n\\n\".join(state[\"documents\"])\n",
        "    system_prompt = \"You are a verification agent. Check if the answer is fully supported by the context, identify any unsupported claims, contradictions, or irrelevance.\"\n",
        "    user_prompt = f\"Question: {state['question']}\\nAnswer: {state['answer']}\\nContext: {context}\\nOutput: 'Supported' or 'Unsupported', followed by a brief explanation.\"\n",
        "    verification = llm_call(system_prompt, user_prompt)\n",
        "    return {\"verification\": verification}\n",
        "\n",
        "# Conditional Edge\n",
        "def relevance_condition(state: State):\n",
        "    rel = state[\"relevance\"].upper()\n",
        "    if rel in [\"CAN_ANSWER\", \"PARTIAL\"]:\n",
        "        return \"generate_answer\"\n",
        "    return \"end\"\n",
        "\n",
        "# Build Graph\n",
        "graph = StateGraph(State)\n",
        "graph.add_node(\"retrieve\", retrieve_documents)\n",
        "graph.add_node(\"check_relevance\", check_relevance)\n",
        "graph.add_node(\"generate_answer\", generate_answer)\n",
        "graph.add_node(\"verify\", verify_answer)\n",
        "\n",
        "graph.set_entry_point(\"retrieve\")\n",
        "graph.add_edge(\"retrieve\", \"check_relevance\")\n",
        "graph.add_conditional_edges(\"check_relevance\", relevance_condition, {\"generate_answer\": \"generate_answer\", \"end\": END})\n",
        "graph.add_edge(\"generate_answer\", \"verify\")\n",
        "graph.add_edge(\"verify\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "# Global variables\n",
        "cache: Dict[str, List[str]] = {}\n",
        "retriever: HybridRetriever = None\n",
        "\n",
        "# Gradio Interface Function\n",
        "def qa_interface(files: List[str], question: str):\n",
        "    global retriever\n",
        "    yield gr.update(value=\"Processing...\")\n",
        "    all_chunks: List[str] = []\n",
        "    for file_path in files:\n",
        "        cache_key = get_cache_key(file_path)\n",
        "        if cache_key not in cache:\n",
        "            cache[cache_key] = process_document(file_path)\n",
        "        all_chunks.extend(cache[cache_key])\n",
        "\n",
        "    retriever = HybridRetriever(all_chunks)  # Rebuild retriever with current chunks\n",
        "\n",
        "    initial_state = {\"question\": question}\n",
        "    result = app.invoke(initial_state)\n",
        "\n",
        "    if \"answer\" not in result:\n",
        "        yield \"No relevant information found in the documents.\"\n",
        "    else:\n",
        "        answer = result[\"answer\"]\n",
        "        verification = result.get(\"verification\", \"Verification not performed.\")\n",
        "        yield f\"**Answer:**\\n{answer}\\n\\n**Verification Report:**\\n{verification}\"\n",
        "\n",
        "# Gradio UI\n",
        "iface = gr.Interface(\n",
        "    fn=qa_interface,\n",
        "    flagging_mode=\"never\",\n",
        "    inputs=[\n",
        "        gr.File(file_count=\"multiple\", label=\"Upload Documents (PDF, DOCX, TXT, MD)\", file_types=[\".pdf\", \".docx\", \".txt\", \".md\"]),\n",
        "        gr.Textbox(label=\"Ask a Question\"),\n",
        "    ],\n",
        "    outputs=gr.Markdown(),\n",
        "    title=\"Document-Based QA System\",\n",
        "    description=\"Upload documents and ask questions. Answers are generated and verified using AI.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "eM-7cMJ7kitH",
        "outputId": "5ec37d77-8a19-4799-c6f9-6f913436bdaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ebc23571823f5ffb2d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ebc23571823f5ffb2d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}